{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f2db60a-3032-486a-b0a9-161ae5722377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MongoDB: stock_data.daily_prices\n",
      "Fetched 5000 records for symbol 'QQQ'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Conv1D, MaxPooling1D, Flatten\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from data_pipeline.mongodb_accessor import StockDataMongoDB\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Load the data\n",
    "mongo_accessor = StockDataMongoDB()\n",
    "data = mongo_accessor.get_stock_data(\"QQQ\")\n",
    "data = data[['close', 'volume']]\n",
    "\n",
    "# Calculate additional features\n",
    "data['price_diff'] = data['close'].diff()  # Difference between consecutive prices\n",
    "data['direction'] = (data['price_diff'] > 0).astype(int)  # Direction label (1 for up, 0 for down)\n",
    "\n",
    "# Drop NaN values created by `diff`\n",
    "data = data.sort_index().dropna()\n",
    "\n",
    "dataset = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0798d239-9fe2-4d09-9b09-50e7bd059297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          close     volume  price_diff  direction\n",
      "0     518.58002   29002300     8.35001          1\n",
      "1     510.23001   36389800    -1.00000          0\n",
      "2     511.23001   29117000    -4.37998          0\n",
      "3     515.60999   34584000    -6.95001          0\n",
      "4     522.56000   33839600    -7.03998          0\n",
      "...         ...        ...         ...        ...\n",
      "4994   37.61000  107270600    -0.02000          0\n",
      "4995   37.63000   78460600     0.41000          1\n",
      "4996   37.22000   95731300    -0.40000          0\n",
      "4997   37.62000   79988700     0.21000          1\n",
      "4998   37.41000   99334600     0.47000          1\n",
      "\n",
      "[4999 rows x 4 columns]\n",
      "0.7547221461812209\n"
     ]
    }
   ],
   "source": [
    "print(data.sort_index())\n",
    "print(sum(data['direction']) / 3653)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39aaae12-99ab-4976-869d-db7004acb9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 0 1 1]\n",
      "Input shape (X): (4939, 60, 3)\n",
      "Output shape (y): (4939,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "features = data[['close', 'volume', 'price_diff']].values\n",
    "target = data['direction'].values  # Predicting direction (classification)\n",
    "\n",
    "# Separate scalers for price, volume, and price_diff\n",
    "price_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "volume_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "diff_scaler = MinMaxScaler(feature_range=(-1, 1))  # Scaled between -1 and 1 for difference\n",
    "\n",
    "scaled_price = price_scaler.fit_transform(features[:, 0].reshape(-1, 1))\n",
    "scaled_volume = volume_scaler.fit_transform(features[:, 1].reshape(-1, 1))\n",
    "scaled_diff = diff_scaler.fit_transform(features[:, 2].reshape(-1, 1))\n",
    "\n",
    "# Combine scaled features\n",
    "scaled_data = np.hstack((scaled_price, scaled_volume, scaled_diff))\n",
    "\n",
    "# Create sequences for LSTM input\n",
    "def create_sequences(data, labels, seq_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        X.append(data[i-seq_length:i])  # Last `seq_length` rows as input\n",
    "        y.append(labels[i])  # Current row's direction as the target\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_length = 60\n",
    "X, y = create_sequences(scaled_data, target, seq_length)\n",
    "print(y)\n",
    "# Ensure shapes are correct\n",
    "print(\"Input shape (X):\", X.shape)  # (n_samples, seq_length, n_features)\n",
    "print(\"Output shape (y):\", y.shape)  # (n_samples,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6be58264-a79d-4f3f-a2e9-9804497a880e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/misaki/.local/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5492 - loss: 0.6909\n",
      "Epoch 2/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5480 - loss: 0.6873\n",
      "Epoch 3/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5511 - loss: 0.6830\n",
      "Epoch 4/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5876 - loss: 0.6771\n",
      "Epoch 5/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5989 - loss: 0.6712\n",
      "Epoch 6/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5643 - loss: 0.6812\n",
      "Epoch 7/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5675 - loss: 0.6751\n",
      "Epoch 8/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5887 - loss: 0.6726\n",
      "Epoch 9/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5930 - loss: 0.6715\n",
      "Epoch 10/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5938 - loss: 0.6656\n",
      "Epoch 11/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6115 - loss: 0.6644\n",
      "Epoch 12/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6071 - loss: 0.6601\n",
      "Epoch 13/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6143 - loss: 0.6603\n",
      "Epoch 14/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6401 - loss: 0.6408\n",
      "Epoch 15/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6202 - loss: 0.6499\n",
      "Epoch 16/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6225 - loss: 0.6471\n",
      "Epoch 17/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6522 - loss: 0.6376\n",
      "Epoch 18/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6409 - loss: 0.6316\n",
      "Epoch 19/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6515 - loss: 0.6251\n",
      "Epoch 20/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6560 - loss: 0.6115\n",
      "Restoring model weights from the end of the best epoch: 20.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fab5071b3d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modified model to accept 2 features\n",
    "# Build the model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dropout, Dense, BatchNormalization\n",
    "\n",
    "def build_model(input_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First convolutional block\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())  # Helps with convergence\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Second convolutional block\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # LSTM layers\n",
    "    model.add(LSTM(units=64, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=32, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Fully connected output layer\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='loss',  # Metric to monitor (e.g., 'val_loss', 'val_accuracy')\n",
    "    patience=3,          # Number of epochs with no improvement before stopping\n",
    "    verbose=1,\n",
    "    restore_best_weights=True  # Restore the weights of the best epoch\n",
    ")\n",
    "\n",
    "# Initialization phase\n",
    "initialization_size = 2000\n",
    "X_initial = X[:initialization_size]\n",
    "y_initial = y[:initialization_size]\n",
    "\n",
    "model = build_model((X_initial.shape[1], 3))  # Note the 2 for 2 features\n",
    "model.fit(X_initial, y_initial, batch_size=32, epochs=20, verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f5e3c-fd84-4d9c-aeb6-2365f6eefc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Directional Backtesting:  89%|███████████████████████████████████████████████████████████████████████████████████▍          | 2660/2998 [01:59<00:13, 25.61it/s, Accuracy=59.14%]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def directional_backtest(model, data, price_scaler, volume_scaler, diff_scaler, seq_length, batch_size=64):\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 1\n",
    "\n",
    "    X_buffer = []\n",
    "    y_buffer = []\n",
    "\n",
    "    with tqdm(total=len(data) - initialization_size  - 1, desc='Directional Backtesting') as pbar:\n",
    "        for i in range(initialization_size , len(data) - 1):\n",
    "            # Prepare sequences for price, volume, and price_diff\n",
    "            current_sequence_price = data[i-seq_length:i, 0].reshape(-1, 1)\n",
    "            current_sequence_volume = data[i-seq_length :i, 1].reshape(-1, 1)\n",
    "            current_sequence_diff = data[i-seq_length :i, 2].reshape(-1, 1)\n",
    "\n",
    "            # Scale features\n",
    "            scaled_price_seq = price_scaler.transform(current_sequence_price)\n",
    "            scaled_volume_seq = volume_scaler.transform(current_sequence_volume)\n",
    "            scaled_diff_seq = diff_scaler.transform(current_sequence_diff)\n",
    "\n",
    "            # Combine and reshape\n",
    "            current_sequence = np.hstack((scaled_price_seq, scaled_volume_seq, scaled_diff_seq))\n",
    "            current_sequence = np.reshape(current_sequence, (1, seq_length, 3))  # Adjust for 3 features\n",
    "\n",
    "            # Make prediction\n",
    "            predicted_value = model.predict(current_sequence, verbose=0)\n",
    "            # print(predicted_value)\n",
    "            up_threshold = 0.75\n",
    "            down_threshold = 0.25\n",
    "            \n",
    "            predicted_up = predicted_value[0][0] >= up_threshold  # Binary decision based on threshold\n",
    "            predicted_down = predicted_value[0][0] <= down_threshold \n",
    "            predicted_no = predicted_value[0][0] < up_threshold and predicted_value[0][0] > down_threshold\n",
    "            \n",
    "            # Get current and next actual prices\n",
    "            current_price = data[i, 0]\n",
    "            next_price = data[i + 1, 0]\n",
    "\n",
    "            # Actual direction\n",
    "            price_went_up = next_price > current_price\n",
    "\n",
    "            if not predicted_no:\n",
    "                # Check directional accuracy\n",
    "                if price_went_up == predicted_up:\n",
    "                    correct_predictions += 1\n",
    "                total_predictions += 1\n",
    "\n",
    "            current_accuracy = (correct_predictions / total_predictions) * 100\n",
    "            pbar.set_postfix({'Accuracy': f'{current_accuracy:.2f}%'})\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Update model with new data (price, volume, diff)\n",
    "            X_buffer.append(current_sequence[0])\n",
    "            y_new = 1 if next_price > current_price else 0  # Binary target\n",
    "            y_buffer.append(y_new)\n",
    "\n",
    "            if len(X_buffer) == batch_size:\n",
    "                X_batch = np.array(X_buffer)\n",
    "                y_batch = np.array(y_buffer)\n",
    "                model.fit(X_batch, y_batch, epochs=20, batch_size=batch_size, verbose=0)\n",
    "                X_buffer = []\n",
    "                y_buffer = []\n",
    "\n",
    "    # Final update for any remaining buffered data\n",
    "    if X_buffer:\n",
    "        X_batch = np.array(X_buffer)\n",
    "        y_batch = np.array(y_buffer)\n",
    "        model.fit(X_batch, y_batch, epochs=20, batch_size=len(X_buffer), verbose=0)\n",
    "\n",
    "    final_accuracy = (correct_predictions / total_predictions) * 100\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nDirectional Prediction Results:\")\n",
    "    print(f\"Total Predictions: {total_predictions}\")\n",
    "    print(f\"Correct Predictions: {correct_predictions}\")\n",
    "    print(f\"Final Accuracy: {final_accuracy:.2f}%\")\n",
    "    print(f\"\\nBaseline Comparison:\")\n",
    "    print(f\"Random Guessing Expected Accuracy: 50.00%\")\n",
    "    print(f\"Model Improvement over Random: {(final_accuracy - 50):.2f}%\")\n",
    "\n",
    "    return final_accuracy\n",
    "\n",
    "# Run the backtest\n",
    "accuracy = directional_backtest(model, dataset, price_scaler, volume_scaler, diff_scaler, seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c015b1a-7820-4026-82ca-75de542c229d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
